{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5671747e-983c-4330-8931-4f987cbfce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2d91b3-f4c2-4052-b813-0f6f420fc448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   Math  Reading  Writing\n",
      "0    48       68       63\n",
      "1    62       81       72\n",
      "2    79       80       78\n",
      "3    76       83       79\n",
      "4    59       64       62\n",
      "\n",
      "Last 5 rows of the dataset:\n",
      "     Math  Reading  Writing\n",
      "995    72       74       70\n",
      "996    73       86       90\n",
      "997    89       87       94\n",
      "998    83       82       78\n",
      "999    66       66       72\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nLast 5 rows of the dataset:\")\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062601ea-3145-47f5-8f36-3d882d5c1d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Math     1000 non-null   int64\n",
      " 1   Reading  1000 non-null   int64\n",
      " 2   Writing  1000 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adec3889-710d-4daf-9cf6-a343edcee7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Description:\n",
      "              Math      Reading      Writing\n",
      "count  1000.000000  1000.000000  1000.000000\n",
      "mean     67.290000    69.872000    68.616000\n",
      "std      15.085008    14.657027    15.241287\n",
      "min      13.000000    19.000000    14.000000\n",
      "25%      58.000000    60.750000    58.000000\n",
      "50%      68.000000    70.000000    69.500000\n",
      "75%      78.000000    81.000000    79.000000\n",
      "max     100.000000   100.000000   100.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Description:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab8c4c0-a3fd-44d2-b516-e0b013e79e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Count:\n",
      "Math       0\n",
      "Reading    0\n",
      "Writing    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values Count:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f027b6-cc68-4f82-b178-f18f0faedd9a",
   "metadata": {},
   "source": [
    "### Splitting the data into Feature (x) and (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6cb436-bf9a-43d6-a344-5fd38d38906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix (X):\n",
      " [[48 68]\n",
      " [62 81]\n",
      " [79 80]\n",
      " [76 83]\n",
      " [59 64]]\n",
      "\n",
      "Target Vector (Y):\n",
      " [63 72 78 79 62]\n"
     ]
    }
   ],
   "source": [
    "X = data[['Math', 'Reading']].values  \n",
    "Y = data['Writing'].values  \n",
    "\n",
    "#selecting the first 5 rows of the feature matrix and target vector using slicing notation. \n",
    "print(\"Feature Matrix (X):\\n\", X[0:5])  \n",
    "print(\"\\nTarget Vector (Y):\\n\", Y[0:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e0708-8e1f-4730-b08d-f48fa0b8b19c",
   "metadata": {},
   "source": [
    ">Usually, Y = W^TX + B\n",
    ">  Since we assume that there is no bias or intercept, Y becomes:\n",
    "> Y = W^TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d39c1a7-91bb-460c-9489-5f5a639f7590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X:  1000\n",
      "\n",
      "\n",
      "Training Features (X) Shape: (800, 2)\n",
      "Testing Features (X) Shape: (200, 2)\n",
      "Training Labels (Y) Shape: (800,)\n",
      "Testing Labels (Y) Shape: (200,)\n",
      "\n",
      "Train Size: 800\n",
      "Test Size: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Length of X: \", len(X))\n",
    "print(\"\\n\")\n",
    "\n",
    "np.random.seed(30) \n",
    "indices = np.random.permutation(len(X))  \n",
    "\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size;\n",
    "\n",
    "X_train = X[indices[:train_size]]\n",
    "X_test = X[indices[train_size:]]\n",
    "Y_train = Y[indices[:train_size]]\n",
    "Y_test = Y[indices[train_size:]]\n",
    "\n",
    "print(\"Training Features (X) Shape:\", X_train.shape)\n",
    "print(\"Testing Features (X) Shape:\", X_test.shape)\n",
    "print(\"Training Labels (Y) Shape:\", Y_train.shape)\n",
    "print(\"Testing Labels (Y) Shape:\", Y_test.shape)\n",
    "print(\"\\nTrain Size:\", train_size)\n",
    "print(\"Test Size:\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce3cc7bd-a46c-43c3-a22a-a8036b0d8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed Further\n",
      "Cost function output: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    This function computes the Mean Squared Error (MSE) cost for Linear Regression.\n",
    "    \n",
    "    Input Parameters:\n",
    "    X: Feature Matrix (n x d) where n is the number of data points and d is the number of features\n",
    "    Y: Target Vector (n,)\n",
    "    W: Weight Vector (d,)\n",
    "    \n",
    "    Output Parameters:\n",
    "    cost: Accumulated mean squared error (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = np.dot(X, W)\n",
    "    \n",
    "    n = len(Y)  \n",
    "    cost = (1 / (2 * n)) * np.sum((y_pred - Y) ** 2)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# with 3 data points and 2 features\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "#target values for the corresponding data points\n",
    "Y_test = np.array([3, 7, 11])\n",
    "\n",
    "#weight vectot for the features\n",
    "W_test = np.array([1, 1])\n",
    "\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "\n",
    "if cost == 0:\n",
    "    print(\"Proceed Further\")\n",
    "else:\n",
    "    print(\"Something went wrong: Reimplement the cost function\")\n",
    "    \n",
    "print(\"Cost function output:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ceba3cf-cf8e-4be4-8bbf-1bf83c007adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix (m x n).\n",
    "    Y (numpy.ndarray): Target vector (m x 1).\n",
    "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
    "    alpha (float): Learning rate.\n",
    "    iterations (int): Number of iterations for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
    "        W_update (numpy.ndarray): Updated parameters (n x 1).\n",
    "        cost_history (list): History of cost values over iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    cost_history = [0] * iterations\n",
    "    \n",
    "    m = len(Y)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values: Y_pred = W.T * X\n",
    "        Y_pred = np.dot(X, W)\n",
    "        \n",
    "        # Step 2: Difference between Hypothesis and Actual Y: loss = Y_pred - Y\n",
    "        loss = Y_pred - Y\n",
    "        \n",
    "        # Step 3: Gradient Calculation: dw = (1/m) * X.T * loss\n",
    "        dw = (1 / m) * np.dot(X.T, loss)\n",
    "        \n",
    "        # Step 4: Update Parameters using Gradient Descent: W = W - alpha * dw\n",
    "        W_update = W - alpha * dw\n",
    "        \n",
    "        # Step 5: Calculate new cost value\n",
    "        cost = cost_function(X, Y, W_update)\n",
    "        cost_history[iteration] = cost\n",
    "        \n",
    "        # Update W to the new W after applying the update rule\n",
    "        W = W_update\n",
    "    \n",
    "    return W_update, cost_history\n",
    "\n",
    "\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) cost for the given parameters W.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix (m x n).\n",
    "    Y (numpy.ndarray): Target vector (m x 1).\n",
    "    W (numpy.ndarray): Parameters vector (n x 1).\n",
    "    \n",
    "    Returns:\n",
    "    cost: The cost (scalar) for the given parameters.\n",
    "    \"\"\"\n",
    "    Y_pred = np.dot(X, W)\n",
    "    m = len(Y)\n",
    "    cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "\n",
    "# np.random.seed(0)  # For reproducibility\n",
    "# X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "# Y = np.random.rand(100)  # 100 target values\n",
    "# W = np.random.rand(3)  # Initial guess for parameters\n",
    "\n",
    "# # Set hyperparameters\n",
    "# alpha = 0.01  # Learning rate\n",
    "# iterations = 1000  # Number of iterations\n",
    "\n",
    "# # Testing the gradient_descent function\n",
    "# final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "\n",
    "# print(\"Final Parameters:\", final_params)\n",
    "# print(\"Cost History:\", cost_history[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c58fc67-af99-41ef-913d-4384abd4d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the Root Mean Squared Error (RMSE).\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- Array of actual (target) dependent variables.\n",
    "    Y_pred -- Array of predicted dependent variables.\n",
    "    \n",
    "    Returns:\n",
    "    rmse -- Root Mean Square Error.\n",
    "    \"\"\"\n",
    "    # Calculate the squared differences between actual and predicted values\n",
    "    squared_errors = (Y - Y_pred) ** 2\n",
    "    \n",
    "    # Calculate the mean of squared errors\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    \n",
    "    # Return the square root of the mean squared error (RMSE)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f6223a-4a85-414b-8a12-6f5e049d0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the R Squared (R²) error.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- Array of actual (target) dependent variables.\n",
    "    Y_pred -- Array of predicted dependent variables.\n",
    "    \n",
    "    Returns:\n",
    "    r2 -- R Squared Error.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the actual values\n",
    "    mean_y = np.mean(Y)\n",
    "    \n",
    "    # Calculate the Total Sum of Squares (SST)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "    \n",
    "    # Calculate the Sum of Squared Residuals (SSR)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    \n",
    "    # Calculate the R Squared (R²) error\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9377b3ed-b1f2-4390-8d3c-1d3f6fb3226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "\n",
    "# Calculate predictions using the optimized parameters\n",
    "Y_pred = X.dot(final_params)  # Assuming a linear model without bias\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_value = rmse(Y, Y_pred)\n",
    "# print(f\"RMSE: {rmse_value}\")\n",
    "\n",
    "# Calculate R²\n",
    "r2_value = r2(Y, Y_pred)\n",
    "# print(f\"R²: {r2_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37577632-4e7c-44c5-984f-a97ba5b19c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights: [0.34811659 0.64614558]\n",
      "Cost History (First 10 iterations): [np.float64(2471.69875), np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.4794892850578), np.float64(889.9583270083234), np.float64(726.8940993009545), np.float64(594.2897260808594), np.float64(486.4552052951635), np.float64(398.7634463599484)]\n",
      "RMSE on Test Set: 5.2798239764188635\n",
      "R-Squared on Test Set: 0.8886354462786421\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to compute RMSE (Root Mean Squared Error)\n",
    "def rmse(Y, Y_pred):\n",
    "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
    "\n",
    "# Function to compute R-Squared\n",
    "def r2(Y, Y_pred):\n",
    "    ss_total = np.sum((Y - np.mean(Y)) ** 2)\n",
    "    ss_residual = np.sum((Y - Y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    m = len(Y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        Y_pred = np.dot(X, W)\n",
    "        cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # Mean Squared Error\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Compute gradient\n",
    "        dw = (1 / m) * np.dot(X.T, (Y_pred - Y))\n",
    "        W -= alpha * dw  # Update weights\n",
    "        \n",
    "    return W, cost_history\n",
    "\n",
    "def main():\n",
    "    # Step 1: Load the dataset\n",
    "    data = pd.read_csv('student.csv')\n",
    "    \n",
    "    # Step 2: Split the data into features (X) and target (Y)\n",
    "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
    "    Y = data['Writing'].values  # Target: Writing marks\n",
    "    \n",
    "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 4: Initialize weights (W) to zeros, learning rate, and number of iterations\n",
    "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
    "    alpha = 0.00001  # Learning rate\n",
    "    iterations = 1000  # Number of iterations for gradient descent\n",
    "    \n",
    "    # Step 5: Perform Gradient Descent\n",
    "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
    "    \n",
    "    # Step 6: Make predictions on the test set\n",
    "    Y_pred = np.dot(X_test, W_optimal)\n",
    "    \n",
    "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
    "    model_rmse = rmse(Y_test, Y_pred)\n",
    "    model_r2 = r2(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Final Weights:\", W_optimal)\n",
    "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
    "    print(\"RMSE on Test Set:\", model_rmse)\n",
    "    print(\"R-Squared on Test Set:\", model_r2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37700622-5225-46d5-a3c8-d04e9f8b5a92",
   "metadata": {},
   "source": [
    "When the learning rate alpha is set to alpha = 0.00001 then the value of R-Squared on test set becomes approximately 0.88, which is closer to 1. \n",
    "We can see that for the first 10 iterations, the cost gradually decreases to 398.76 starting from 2471.69. Gradient descent seems to be working as expected. A well-chose alpha, the value of learning rate might have contributed to its nature. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
